{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_Final",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leoliu00529/CodeSnap/blob/master/CNN_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_-8qa_tAoAL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        },
        "outputId": "fca9bbd3-93d9-446c-b7fb-64833042b42a"
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, Dense, Flatten, MaxPooling2D, Dropout, Activation\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# in_xtrain = open('xtrain', 'rb')\n",
        "# x_train = pickle.load(in_xtrain)\n",
        "# in_xtrain.close()\n",
        "\n",
        "# in_ytrain = open('ytrain', 'rb')\n",
        "# y_train = pickle.load(in_ytrain)\n",
        "# in_ytrain.close()\n",
        "\n",
        "input_shape = (116, 300, 1)\n",
        "\n",
        "#Model:\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(200, (2, 2), padding='same', strides=1, \n",
        "                 activation='relu', input_shape=input_shape))\n",
        "model.add(Conv2D(200, (3, 3), activation='relu'))\n",
        "model.add(Conv2D(200, (4, 4), activation='relu'))\n",
        "model.add(Conv2D(200, (5, 5), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(4))\n",
        "model.add(Activation('softmax'))\n",
        "print(model.summary())\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.RMSprop(lr=0.0001),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          epochs=3,\n",
        "          verbose=1,\n",
        "          batch_size=4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_9 (Conv2D)            (None, 116, 300, 200)     1000      \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 114, 298, 200)     360200    \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 111, 295, 200)     640200    \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 107, 291, 200)     1000200   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 53, 145, 200)      0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 1537000)           0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 100)               153700100 \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 4)                 404       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 4)                 0         \n",
            "=================================================================\n",
            "Total params: 155,702,104\n",
            "Trainable params: 155,702,104\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/3\n",
            "5572/5786 [===========================>..] - ETA: 25s - loss: 1.0634 - acc: 0.4937"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98oP6vcUTNN5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7cecef16-0913-41b0-8430-661b6dddc4fe"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzAEKqoVTMG7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzI528Vciact",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "53bfcf0f-10f2-45bd-9718-5162d7b2e6a2"
      },
      "source": [
        "!pip3 install indexer\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting indexer\n",
            "  Downloading https://files.pythonhosted.org/packages/c7/2f/49ea001ccc81502fe790c6077ca0cf9c4dc98ce160e1b1225a8c881b53b1/indexer-0.6.2.tar.gz\n",
            "\u001b[31mERROR: Command errored out with exit status 1: python setup.py egg_info Check the logs for full command output.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USjJIbKbin_t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "outputId": "30cc952c-b567-48d5-c1ac-ac82298a8840"
      },
      "source": [
        "import spellchecker\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-64f2c252d1da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspellchecker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spellchecker/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# -*- coding: utf-8 -*-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m  \u001b[0mspellchecker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSpellchecker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgetInstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spellchecker/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDictionaryIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangdetect\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_detect_lang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'indexer'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZaMXgHeCBDJ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from zipfile import ZipFile\n",
        "zip = ZipFile('xtrain.zip')\n",
        "zip.extractall()\n",
        "zip1 = ZipFile('ytrain.zip')\n",
        "zip1.extractall()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-ujVUdpdvxu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1368
        },
        "outputId": "453b4a6e-28a2-4cea-ca77-2128ace0462c"
      },
      "source": [
        "import keras\n",
        "from keras import backend as K\n",
        "import numpy\n",
        "import nltk\n",
        "import numpy as np\n",
        "import re\n",
        "from gensim.models import Word2Vec\n",
        "import tensorflow as tf\n",
        "from keras.backend.tensorflow_backend import set_session\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, Dense, Flatten, MaxPooling2D, Dropout, Activation\n",
        "import pickle\n",
        "\n",
        "nltk.download('punkt')\n",
        "BATCH_SIZE = 50\n",
        "DENSE_DROP_OUT = 0.25\n",
        "LEARNING_RATE = 0.0001\n",
        "EMBEDDING_DIM = 300\n",
        "EMBEDDING_LENGTH = 116 #Max sentence length. Apply padding when needed \n",
        "NUM_CLASSES = 2 #or 3, when dealing with relation that has an inverse\n",
        "\n",
        "\n",
        "#learning the vocab and word2vec embedding\n",
        "all_words = []\n",
        "with open('''train.txt''') as train_file:\n",
        "  for line in train_file:\n",
        "      if line != '\\n' and not line.startswith('REL:'):\n",
        "          all_words.append(re.sub(' .*\\n', '', line.lower()))\n",
        "  train_file.close()         \n",
        "with open('''test_g.txt''') as test_file:\n",
        "  for line in test_file:\n",
        "      if line != '\\n' and not line.startswith('REL:'):\n",
        "          all_words.append(re.sub(' .*\\n', '', line.lower()))\n",
        "  test_file.close()\n",
        "          \n",
        "all_words = [nltk.word_tokenize(word) for word in all_words]\n",
        "wordembedding = Word2Vec(all_words, size=300, min_count=1)\n",
        "vocab = wordembedding.wv.vocab\n",
        "\n",
        "RELATION_TYPE = 'overlap'\n",
        "x_train = []\n",
        "y_train = []\n",
        "x_test = []\n",
        "y_test = []\n",
        "\n",
        "#Longest sentence: 106\n",
        "#Thus input: 116 * 300\n",
        "#two helper function to process the input \n",
        "\n",
        "# this function appends the a 116 * 300 word embedding to the train (or test) data\n",
        "def createInstance(x_train, y_train, fs, fl, ts, tl, label, wordSequence):\n",
        "    array = []\n",
        "    length = int(112 - len(wordSequence))\n",
        "    for each in range(0, int(length/2)):\n",
        "        array.append(np.zeros(300))\n",
        "    for i, word in enumerate(wordSequence):\n",
        "        # if i == fs or i == fs + fl or i == ts or i == ts + tl:\n",
        "        #     array.append(np.zeros(300))\n",
        "        try: array.append(wordembedding.wv[word])\n",
        "        except: array.append(np.zeros(300))\n",
        "    for each in range(0, length - (int(length / 2))):\n",
        "        array.append(np.zeros(300))\n",
        "    array.insert(fs, np.zeros(300))\n",
        "    array.insert(fs + fl, np.zeros(300))\n",
        "    array.insert(ts, np.zeros(300))\n",
        "    array.insert(ts + tl, np.zeros(300))\n",
        "    x_train.append(array)\n",
        "    if label:\n",
        "        y_train.append(0)\n",
        "    else:\n",
        "        y_train.append(1)\n",
        "\n",
        "\n",
        "# this function reads a segment of text from the raw data and extract instances \n",
        "# of relations that we concern with. In this case, we only concern with 'overlap'\n",
        "# relation. And since 'overlap' does not have an inverse relation (unlike \n",
        "# 'contains' or 'before'), the label would be either [1, 0] or [0, 1]\n",
        "def Sentence(wholetext, train):\n",
        "    wholetext = wholetext.splitlines(True)\n",
        "    wordSequence = []\n",
        "    relationLine = []\n",
        "    for textline in wholetext:\n",
        "        if textline != '\\n':\n",
        "            if not textline.startswith('REL:'):\n",
        "                wordSequence.append(str(re.sub(' .*\\n', '', textline.lower())))\n",
        "            else:\n",
        "                relationLine.append(str(re.sub('\\n', '', textline.lower())))\n",
        "    for textline in relationLine:\n",
        "        if not '''/''' in textline:\n",
        "            textline = re.sub('rel: ', '', textline.lower())\n",
        "            parsed_line= re.findall('[0-9][0-9]*', textline)\n",
        "            fromPos = int(parsed_line[0])\n",
        "            secondPart = re.sub('.*?;', '', textline, 1)\n",
        "            toPos = int(re.findall('[0-9][0-9]*', secondPart)[0])\n",
        "            from_length = re.findall('.*?;', textline)[0].count(',') + 1\n",
        "            to_length = re.findall('.*?;', textline)[1].count(',') + 1\n",
        "            label = False\n",
        "            if re.sub('.*;', '', textline) == RELATION_TYPE:\n",
        "                label = True\n",
        "                # createInstance(x_train, y_train, self.wordSequence[fromPos:fromPos + from_length],\n",
        "                #                self.wordSequence[toPos:toPos + to_length], label)\n",
        "            if train:\n",
        "                createInstance(x_train, y_train, fromPos, from_length, toPos, to_length, label, wordSequence)\n",
        "            else:\n",
        "                createInstance(x_test, y_test, fromPos, from_length, toPos, to_length, label, wordSequence)\n",
        "\n",
        "\n",
        "# read the training data. For testing purpose, only deal with first 50000 lines \n",
        "# train1 is a tiny sample of the original training data, containging roughly 100 lines\n",
        "\n",
        "print('Generating training data')\n",
        "\n",
        "feed = ''\n",
        "with open('''train.txt''') as f:\n",
        "  for i, line in enumerate(f):\n",
        "    if i < 200000:\n",
        "      if not line.startswith('\\n'):\n",
        "        feed += line\n",
        "      else:\n",
        "        if feed != '':\n",
        "          Sentence(feed, True)\n",
        "          feed = ''\n",
        "  f.close()\n",
        "\n",
        "# read the testing_ground_truth data\n",
        "# test1 is a tiny sample of the original testing data, containging roughly 100 lines\n",
        "\n",
        "print('Generating testing data')\n",
        "\n",
        "feed = ''\n",
        "with open('''test_g.txt''') as f:\n",
        "  for i, line in enumerate(f):\n",
        "    if i < 1500:\n",
        "      if not line.startswith('\\n'):\n",
        "        feed += line\n",
        "      else:\n",
        "        if feed != '':\n",
        "          Sentence(feed, False)\n",
        "          feed = ''\n",
        "  f.close()\n",
        "\n",
        "  \n",
        "\n",
        "# dump all unnecessary data into dist to save spaces\n",
        "file_wordembedding = open('word', 'wb')\n",
        "pickle.dump(wordembedding, file_wordembedding)\n",
        "file_wordembedding.close()\n",
        "\n",
        "# reshape the data into tensor and adding a dummy dimension\n",
        "print('Reshaping data')\n",
        "\n",
        "x_train = np.array(x_train)\n",
        "print('x_train: ')\n",
        "print(type(x_train))\n",
        "print(x_train.shape)\n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0], 116, 300, 1)\n",
        "y_train = keras.utils.to_categorical(y_train, 2)\n",
        "\n",
        "# file_xtrain = open('xtrain', 'wb')\n",
        "# pickle.dump(x_train, file_xtrain)\n",
        "# file_xtrain.close()\n",
        "\n",
        "\n",
        "x_test = np.array(x_test)\n",
        "print('x_test: ')\n",
        "print(type(x_test))\n",
        "print(x_test.shape)\n",
        "print(x_test.shape)\n",
        "x_test = x_test.reshape(x_test.shape[0], 116, 300, 1)\n",
        "y_test = keras.utils.to_categorical(y_test, 2)\n",
        "\n",
        "\n",
        "# After this point, all four inputs should be tensors in the form of \n",
        "# (sample_num, 116, 300, 1)\n",
        "\n",
        "input_shape = (EMBEDDING_LENGTH, EMBEDDING_DIM, 1)\n",
        "\n",
        "\n",
        "\n",
        "# The following paragraph is directly paraphrased from the paper:\n",
        "\n",
        "# In the CNN-based model, the\n",
        "# embedding layer is followed by a convolution layer\n",
        "# that applies convolving filters of sizes (2, 3, 4, 5) with 200 filters each to\n",
        "# extract n-gram-like features, which are then pooled\n",
        "# by a max-pooling layer.\n",
        "# Output of the max-pool layer is fed into a fully connected\n",
        "# dense layer (drop out rate 0.25), which is followed by the final softmax\n",
        "# layer outputting the probability distribution over\n",
        "# the three possible classes for the input. (RMSprop optimizer)\n",
        "\n",
        "# Some assumptions I have to made: stride size (I assume 1 for conv layer and\n",
        "# 2 for pooling layer), activation (I assume relu), \n",
        "#loss (I assume sparse crossentropy)\n",
        "\n",
        "\n",
        "# Building the CNN \n",
        "\n",
        "\n",
        "print('Building the model')\n",
        "model = Sequential()\n",
        "\n",
        "# Four 2D convolution layers\n",
        "model.add(Conv2D(200, (2, 2), padding='same', strides=1, \n",
        "                 activation='relu', input_shape=input_shape))\n",
        "model.add(Conv2D(200, (3, 3), activation='relu'))\n",
        "model.add(Conv2D(200, (4, 4), activation='relu'))\n",
        "model.add(Conv2D(200, (5, 5), activation='relu'))\n",
        "\n",
        "# One max pooling \n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "\n",
        "# Dense layer with dropout = 0.25\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dropout(DENSE_DROP_OUT))\n",
        "model.add(Dense(2))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "print('Compiling the model')\n",
        "print(model.summary())\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.RMSprop(lr=LEARNING_RATE),\n",
        "              metrics=['accuracy'])\n",
        "print('Running the model')\n",
        "model.fit(x_train, y_train,\n",
        "          epochs=3,\n",
        "          verbose=1,\n",
        "          batch_size=2,\n",
        "          validation_data=(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Generating training data\n",
            "Generating testing data\n",
            "Reshaping data\n",
            "x_train: \n",
            "<class 'numpy.ndarray'>\n",
            "(20591, 116, 300)\n",
            "x_test: \n",
            "<class 'numpy.ndarray'>\n",
            "(149, 116, 300)\n",
            "(149, 116, 300)\n",
            "Building the model\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Compiling the model\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 116, 300, 200)     1000      \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 114, 298, 200)     360200    \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 111, 295, 200)     640200    \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 107, 291, 200)     1000200   \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 53, 145, 200)      0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 1537000)           0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               153700100 \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 2)                 202       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 2)                 0         \n",
            "=================================================================\n",
            "Total params: 155,701,902\n",
            "Trainable params: 155,701,902\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Running the model\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 20591 samples, validate on 149 samples\n",
            "Epoch 1/3\n",
            "  234/20591 [..............................] - ETA: 1:08:38 - loss: 0.6969 - acc: 0.6197"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-50643b41bf39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    229\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m           validation_data=(x_test, y_test))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2c8O4NFBnmf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}